{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd85e39d",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed8c591",
   "metadata": {},
   "source": [
    "1. What is a learner said to do when it outputs a classifier that is 100% accurate on the training\n",
    "data but only 50% accurate on test data, when in fact it could have output one that is 75%\n",
    "accurate on both?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371b07b3",
   "metadata": {},
   "source": [
    "When a learner outputs a classifier that is 100% accurate on the training data but only 50% accurate on test data, it is said to have overfit the training data. Overfitting occurs when a model is too complex and captures not only the signal in the data but also the noise, which can result in poor generalization performance on unseen data.\n",
    "\n",
    "In this case, the learner has learned the training data too well and has memorized the training examples instead of learning the underlying patterns and relationships in the data. As a result, the model may not be able to generalize well to new, unseen data, which can result in poor performance on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21556c25",
   "metadata": {},
   "source": [
    "2. In stochastic gradient descent, each pass over the dataset requires the same number of arithmetic\n",
    "operations, whether we use mini-batches of size one or size 1000. Why can it nevertheless\n",
    "be more computationally efficient to use mini-batches of size 1000?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e254a1",
   "metadata": {},
   "source": [
    "One of the main advantages of MBGD over SGD is that it can take advantage of hardware optimization of matrix operations, especially when using graphics processing units (GPUs) as they are designed to perform large-scale matrix operations in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910c3acc",
   "metadata": {},
   "source": [
    "3. Below Figure 1 shows the level curves in the weight space of a cost function C which we are\n",
    "trying to minimize. The current weight vector is marked by an x. Sketch the gradient descent\n",
    "update."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8decf914",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/IrRtZPz.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d3f975",
   "metadata": {},
   "source": [
    "The next step goes outwards (i.e. approaching the line C=1) as the cost function decreases in the outward direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d748974",
   "metadata": {},
   "source": [
    "4. Suppose we want to train a perceptron (refer to Figure 2) with weights w1 and w2 and a fixed\n",
    "bias b = âˆ’1 Sketch the constraints in weight space corresponding to the following training\n",
    "cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e747ef3f",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/qv50Mvx.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
