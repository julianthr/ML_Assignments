{"cells":[{"cell_type":"code","execution_count":null,"id":"5c3a7b6e","metadata":{"id":"5c3a7b6e"},"outputs":[],"source":["import numpy as np\n","import librosa\n","from pathlib import Path\n","import os\n","import tarfile\n","import pickle\n","import math\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from tqdm import tqdm"]},{"cell_type":"code","source":["from google.colab import drive\n","\n","# mount to Google Drive \n","drive.mount(\"/content/drive\")\n","\n","# save the most common part of file path\n","root_path = \"/content/drive/My Drive/Audio_Mood_Classification\""],"metadata":{"id":"KQUaywH1b4pC"},"id":"KQUaywH1b4pC","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"3eeb4ba5","metadata":{"id":"3eeb4ba5"},"outputs":[],"source":["# load labels dictionary\n","with open(f\"{root_path}/dictionary/tracks.pkl\", \"rb\") as f:\n","    tracks = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"id":"5be131dc","metadata":{"scrolled":true,"id":"5be131dc","outputId":"96bcb8ca-0243-4b75-ef4e-7a19bbda84f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["7087\n"]}],"source":["# check amount of tracks in pickle file\n","print(len(list(tracks.items())))"]},{"cell_type":"code","execution_count":null,"id":"418be2d8","metadata":{"id":"418be2d8","outputId":"e685da0b-be11-4cc2-abdb-e4944eb06a0b"},"outputs":[{"name":"stderr","output_type":"stream","text":[" 40%|████      | 7480/18486 [1:22:23<40:21,  4.55it/s]   "]}],"source":["# loading loop, takes ~2 min per batch (99 batches in total)\n","\n","files = os.listdir(f\"{root_path}/audio_files\")\n","\n","i=0\n","populated_IDS = []\n","\n","\n","for file in tqdm(files):\n","\n","    ID = file.split(\".\")[0] #do with enumerate\n","    i += 1\n","\n","    if int(ID) in tracks.keys():\n","\n","        #storing the raw audio file\n","        y, sr = librosa.load(f\"{root_path}/audio_files/{file}\", sr=22050)\n","        ##transforming the audio file into MFCC with 20 coefficients\n","        #y_mfcc = librosa.feature.mfcc(y=y, sr=sr)\n","\n","        #code to access the ID, where ID is the id code\n","        tracks[int(ID)][\"raw\"] = y\n","        #tracks[int(ID)][\"mfcc\"] = y_mfcc\n","\n","        populated_IDS.append(ID)    "]},{"cell_type":"code","execution_count":null,"id":"b8965cc0","metadata":{"id":"b8965cc0","outputId":"f97b62d5-6cea-4b7e-9274-535fd729b66d"},"outputs":[{"name":"stdout","output_type":"stream","text":["['1000086', '1001307', '1001312', '1002052', '1002753', '1002756', '1002758', '1003417', '1003418', '1003517']\n","3246\n"]}],"source":["print(populated_IDS[:10])\n","print(len(populated_IDS))"]},{"cell_type":"code","execution_count":null,"id":"d4f757df","metadata":{"scrolled":true,"id":"d4f757df","outputId":"75d0b220-b14d-40e4-86bf-098f7d28b09d"},"outputs":[{"data":{"text/plain":["{'artist_id': 366613,\n"," 'album_id': 120946,\n"," 'path': '02/1028902.mp3',\n"," 'duration': 239.3,\n"," 'mood/theme': {'sad'}}"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["tracks[1028902]"]},{"cell_type":"markdown","id":"e9ce1a26","metadata":{"id":"e9ce1a26"},"source":["### Preprocess raw audio data"]},{"cell_type":"code","execution_count":null,"id":"e002a2d4","metadata":{"id":"e002a2d4"},"outputs":[],"source":["# cut the raw audio data in 30 second windows\n","\n","print(\"30 seconds of the audio at a sample rate of 22050 results in\", 30*22050, \"elements.\")\n","\n","for key in tqdm(populated_IDS):\n","    middle = (int(math.ceil(len(tracks[int(key)][\"raw\"]) / 2))) - 1\n","    tracks[int(key)][\"raw_30s\"] = tracks[int(key)][\"raw\"][middle-330750:middle+330750]    "]},{"cell_type":"code","execution_count":null,"id":"90553d4d","metadata":{"id":"90553d4d"},"outputs":[],"source":["print(len(tracks[1028902][\"raw_30s\"]))"]},{"cell_type":"code","execution_count":null,"id":"0e79462d","metadata":{"id":"0e79462d"},"outputs":[],"source":["# even length test\n","\n","print(len(tracks[1028902][\"raw\"]))\n","\n","middle = int(math.ceil(len(tracks[1028902][\"raw\"]) // 2)) - 1\n","print(middle)\n","\n","test = tracks[1028902][\"raw\"][middle-330750:middle+330750]\n","print(len(test))"]},{"cell_type":"code","execution_count":null,"id":"887a7481","metadata":{"id":"887a7481"},"outputs":[],"source":["# odd length test\n","\n","print(len(tracks[1088002][\"raw\"]))\n","\n","middle = int(math.ceil(len(tracks[1088002][\"raw\"]) // 2)) - 1\n","print(middle)\n","\n","test = tracks[1088002][\"raw\"][middle-330750:middle+330750]\n","print(len(test))"]},{"cell_type":"code","execution_count":null,"id":"1d484d2c","metadata":{"id":"1d484d2c"},"outputs":[],"source":["# # create 2D numpy array of raw audio wave arrays\n","\n","# X_list = [] \n","\n","# for key in populated_IDS:\n","#     X_list.append(tracks[int(key)][\"raw_30s\"])\n","\n","# X = np.vstack(X_list)\n","\n","# print(X)"]},{"cell_type":"code","execution_count":null,"id":"852655df","metadata":{"scrolled":true,"id":"852655df"},"outputs":[],"source":["# create numpy array of labels by multi-hot encoding the labels\n","\n","labels_list = []\n","\n","for key in populated_IDS:\n","    labels_list.append(list(tracks[int(key)][\"mood/theme\"]))\n","\n","mlb = MultiLabelBinarizer()\n","y_hot = mlb.fit_transform(labels_list)\n","\n","print(mlb.classes_)\n","print(y_hot)\n"]},{"cell_type":"markdown","id":"38e498df","metadata":{"id":"38e498df"},"source":["### Preprocess MFCC data"]},{"cell_type":"code","execution_count":null,"id":"e1f2d31d","metadata":{"id":"e1f2d31d"},"outputs":[],"source":["# print(tracks[1028902][\"mfcc\"].shape) # MFCCs have 20 coefficients, number of frames (segment of the audio signal) depends on the length of the track, in this case 10306\n"]},{"cell_type":"code","execution_count":null,"id":"6e04fa71","metadata":{"id":"6e04fa71"},"outputs":[],"source":["# recalculate the MFCCs with the raw 30s data and store it in the tracks dictionary\n","    \n","for key in tqdm(populated_IDS):\n","\n","    # compute mfcc, sample rate: 22050, number of coefficients: 20, number of frames = 1292, frame size (hop_length) = 512\n","    y_mfcc_30s = librosa.feature.mfcc(y=tracks[int(key)][\"raw_30s\"], sr=22050, hop_length=512)\n","    # store in tracks dictionary\n","    tracks[int(key)][\"mfcc_30s\"] = y_mfcc_30s"]},{"cell_type":"code","execution_count":null,"id":"a38efa8d","metadata":{"scrolled":false,"id":"a38efa8d"},"outputs":[],"source":["# rows represent the number of mel-frequency cepstral coefficients extracted per frame\n","# columns represent the number of frames in the audio signal (frame size 512)\n","\n","print(tracks[1028902][\"mfcc_30s\"][:5])\n","print(tracks[1028902][\"mfcc_30s\"].shape)\n","print(tracks[1053502][\"mfcc_30s\"].shape)"]},{"cell_type":"code","execution_count":null,"id":"6a47913b","metadata":{"id":"6a47913b"},"outputs":[],"source":["print(populated_IDS)"]},{"cell_type":"code","execution_count":null,"id":"588a62eb","metadata":{"id":"588a62eb"},"outputs":[],"source":["# the 30s MFCCs all have the same length, i.e., same number of frames\n","\n","for key in populated_IDS[:10]:\n","    print(key + \":\")\n","    print(tracks[int(key)][\"mfcc_30s\"].shape)"]},{"cell_type":"code","execution_count":null,"id":"367500c3","metadata":{"id":"367500c3"},"outputs":[],"source":["# stack the 30s MFCCs of all the audio tracks inside a numpy array, resulting array has dimensions (n_samples, n_coeff, n_frames)\n","\n","mfccs_list = [] \n","\n","for key in populated_IDS:\n","    mfccs_list.append(tracks[int(key)][\"mfcc_30s\"])\n","\n","mfccs = np.stack(mfccs_list, axis=0)\n","\n","print(mfccs[:3, :, :])\n","print(mfccs.shape)\n"]},{"cell_type":"markdown","id":"35edb2bb","metadata":{"id":"35edb2bb"},"source":["To feed MFCCs into a CNN, we need to have the following dimensions:\n","\n","1. Number of samples: This is the number of examples we have in our dataset.\n","1. Number of frames: This is the number of time steps or frames we have for each example.\n","1. Number of MFCC coefficients: This is the number of MFCC coefficients we have for each time step.\n","1. Number of channels: This is 1 for grayscale images and 3 for RGB images."]},{"cell_type":"code","execution_count":null,"id":"a6636c47","metadata":{"scrolled":true,"id":"a6636c47"},"outputs":[],"source":["# transpose the dimensions of the mfccs array to the order specified above, dimensions (n_samples, n_frames, n_coeff)\n","mfccs = mfccs.transpose(0, 2, 1)\n","print(mfccs.shape)"]},{"cell_type":"code","execution_count":null,"id":"f5b05dc6","metadata":{"id":"f5b05dc6"},"outputs":[],"source":["# scale each MFCC to a range between 0 and 1 across all samples and frames to ensure that the model can learn the relevant patterns using the whole data)\n","\n","# reshape the MFCCs to a 2D array for scaling\n","mfccs_2d = mfccs.reshape(-1, mfccs.shape[-1])\n","\n","print(mfccs_2d)\n","print(mfccs_2d.shape)\n","\n","# scale the MFCCs\n","mfccs_scaled = MinMaxScaler().fit_transform(mfccs_2d)\n","\n","# reshape the scaled MFCCs back to the original shape\n","mfccs_scaled = mfccs_scaled.reshape(mfccs.shape)\n","\n","print(mfccs_scaled)\n","print(mfccs_scaled.shape)\n"]},{"cell_type":"code","execution_count":null,"id":"2087e66a","metadata":{"id":"2087e66a"},"outputs":[],"source":["# add n_channels = 1 to the MFCCs data, dimensions: (n_samples, n_frames, n_coeff, n_channels)\n","mfccs_scaled = np.expand_dims(mfccs_scaled, axis=-1)\n","print(mfccs_scaled.shape)"]},{"cell_type":"code","execution_count":null,"id":"67f97082","metadata":{"id":"67f97082"},"outputs":[],"source":["# split the MFCCs in train:validation:test in the ratio 60:20:20\n","\n","# split the MFCCs into train and test sets\n","X_train_, X_test, y_train_, y_test = train_test_split(mfccs_scaled, y_hot, test_size=0.2, random_state=42)\n","\n","# split training data into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(X_train_, y_train_, test_size=0.25, random_state=42)"]},{"cell_type":"code","execution_count":null,"id":"f1d7d186","metadata":{"id":"f1d7d186"},"outputs":[],"source":["print(\"X_train:\")\n","print(X_train.shape)\n","print(X_train[:1])\n","print()\n","print(\"X_val:\")\n","print(X_val[:1])\n","print()\n","print(\"X_test:\")\n","print(X_test[:1])\n","print()\n","print(\"y_train:\")\n","print(y_train.shape)\n","print(y_train[:1])\n","print()\n","print(\"y_val:\")\n","print(y_val[:1])\n","print()\n","print(\"y_test:\")\n","print(y_test[:1])"]},{"cell_type":"code","execution_count":null,"id":"9c0fb7c8","metadata":{"id":"9c0fb7c8"},"outputs":[],"source":["# store the train, validation and test set of 30s MFCCs\n","\n","files = [X_train_mfcc30s, X_val_mfcc30s, X_test_mfcc30s, y_train_mfcc30s, y_val_mfcc30s, y_test_mfcc30s]\n","\n","file_names = [\"X_train.npy\",\n","              \"X_val.npy\",\n","              \"X_test.npy\",\n","              \"y_train.npy\",\n","              \"y_val.npy\",\n","              \"y_test.npy\"]\n","\n","for file, file_name in zip(files, file_names):\n","    # np.save(f\"./mfcc30s/{file_name}\", file)\n","    np.save(f\"{root_path}/train_test_data_full/{file_name}\", file) # path to new folder to store MFCCs of full dataset"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}